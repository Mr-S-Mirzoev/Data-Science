{"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"file_extension":".py","pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","version":"3.7.7"},"notebookId":"ed86a541-3574-4435-81cf-6cf077f60be3"},"cells":[{"cell_type":"markdown","source":"## Seminar 5 - Dense Prediction","metadata":{"cellId":"f12hlbeiplvt94g5r23l2o"}},{"cell_type":"markdown","source":"Your task is to convert image classification network into fully-convolutional network that predicts value for every patch on image.\n\nWhat we have:\n* network trained to predict whether the central pixel of patch of size 114x114 belong to class 'road'\n* image that we want to segment\n\nLet's firstly look on the data","metadata":{"cellId":"jf8ijqxa1dotxk58ts2tle"}},{"cell_type":"code","source":"import scipy as sp\nimport scipy.misc\nimport skimage.io\nimport skimage.transform\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n%matplotlib inline","metadata":{"cellId":"5nobci51sb3e33308ecnw5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if running in colab or datasphere, uncomment this:\n#os.system(\"wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/spring21/seminar05-dense_prediction/um_000081-pred.png -O um_000081-pred.png\")\n#os.system(\"wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/spring21/seminar05-dense_prediction/layer_wrappers.py -O layer_wrappers.py\")\n#os.system(\"wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/spring21/seminar05-dense_prediction/um_000081.png -O um_000081.png\")\n#os.system(\"wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/spring21/seminar05-dense_prediction/um_road_000081.png -O um_road_000081.png\")","metadata":{"cellId":"24yiz07nfclvswa5gy2w9"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install scikit-image","metadata":{"cellId":"q88rtltbzco2jdc3aw954k"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.system(\"wget https://www.dropbox.com/s/429eqmyi3dbhzrg/model2.npz?dl=0 -O model2.npz\")\n# alternative link: https://yadi.sk/d/I1kHDWZTnH4DAw (for manual downloading only)","metadata":{"cellId":"w05atkgdtwifnofswj3z7l"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_image(img_name, gt_name=None):\n    IMG_HEIGHT = 256\n    im = skimage.io.imread(img_name)\n    im = skimage.transform.rescale(im, IMG_HEIGHT * 1. / im.shape[0], multichannel=True)\n    im = skimage.img_as_ubyte(im)\n    if gt_name is not None:\n        gt = (skimage.io.imread(gt_name)[:,:,-1]==255).astype(np.uint8)*255\n        gt = skimage.transform.rescale(gt, IMG_HEIGHT * 1. / gt.shape[0], order=0, preserve_range=True)\n        gt = (gt > 0).astype(np.uint8)\n        return im, gt\n    return im\n\ndef make_blending(img, labels, alpha=0.5):\n    colors = np.array([[0,0,0], [0,255,0]], np.uint8)\n    return (img*alpha + colors[labels.astype(np.int32)]*(1. - alpha)).astype(np.uint8)","metadata":{"cellId":"xjxp981xwnn73x76cncace"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.subplot(3,1,1)\nim, gt = read_image('./um_000081.png', './um_road_000081.png')\nplt.imshow(im)\nplt.title('Source image')\nplt.xticks([]); plt.yticks([])\nplt.subplot(3,1,2)\nplt.imshow(make_blending(im, gt))\nplt.title('Ground truth')\nplt.xticks([]); plt.yticks([])\nplt.subplot(3,1,3)\npred = skimage.io.imread('./um_000081-pred.png')\nplt.imshow(pred)\nplt.title('Expected prediction')\nplt.xticks([]); plt.yticks([])\n","metadata":{"cellId":"wu9blsee3peo6n71hxdae"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Semantic image segmentation problem could be considered as a problem of prediction label for the central pixel in image patch of predefined size. It allows us to use a lot of NN archtectures specifically designed for image classification (thanks to ImageNet Competition)","metadata":{"cellId":"5v9tnb5bp3e3qp60eqlbi"}},{"cell_type":"code","source":"def get_valid_patches(img_shape, patch_size, central_points):\n    start = central_points - patch_size/2\n    end = start + patch_size\n    mask = np.logical_and(start >= 0, end < np.array(img_shape))\n    mask = np.all(mask, axis=-1)\n    return mask\n\ndef extract_patches(img, mask, n_pos=64, n_neg=64, patch_size=100):\n    res = []\n    labels = []\n    pos = np.argwhere(mask > 0)\n    accepted_patches_mask = get_valid_patches(np.array(img.shape[:2]), patch_size, pos)\n    pos = pos[accepted_patches_mask]\n    np.random.shuffle(pos)\n    for i in range(n_pos):\n        start = pos[i] - patch_size//2\n        end = start + patch_size\n        res.append(img[start[0]:end[0], start[1]:end[1]])\n        labels.append(1)\n        \n    neg = np.argwhere(mask == 0)\n    accepted_patches_mask = get_valid_patches(np.array(img.shape[:2]), patch_size, neg)\n    neg = neg[accepted_patches_mask]\n    np.random.shuffle(neg)\n    for i in range(n_neg):\n        start = neg[i] - patch_size//2\n        end = start + patch_size\n        res.append(img[start[0]:end[0], start[1]:end[1]])\n        labels.append(0)\n    return np.array(res), np.array(labels)","metadata":{"cellId":"v4olryidiwja1xjymssvfi"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"patches, labels = extract_patches(im, gt, 32,32, patch_size=114)","metadata":{"cellId":"31mwmydpk9l8jobxhaequc"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Road-centered patches","metadata":{"cellId":"qi8ittez6zlifl8obwipf"}},{"cell_type":"code","source":"plt.figure(figsize=(15,3))\nfor i in range(5):\n    plt.subplot(1,5,i+1)\n    plt.imshow(patches[i])\n    plt.xticks([]); plt.yticks([])","metadata":{"cellId":"qlwmbr2cf8sbhpwrb4z6l"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Non-road-centered patches","metadata":{"cellId":"wtltcply7ffugd01ntps"}},{"cell_type":"code","source":"plt.figure(figsize=(15,3))\nfor i in range(1,6):\n    plt.subplot(1,5,i)\n    plt.imshow(patches[-i])\n    plt.xticks([]); plt.yticks([])","metadata":{"cellId":"p00zrphwnmpc981zphkse"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here is our pretrained network","metadata":{"cellId":"ipftiqh421g0tv3iikztah"}},{"cell_type":"code","source":"with np.load('./model2.npz', encoding='latin1', allow_pickle=True) as f:\n    weights = f['state'].tolist()  # getting np.array content; it's dict in fact, not list","metadata":{"cellId":"suiewmn1kyr2om25kb28uf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from layer_wrappers import *\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n    \ndef create_network(weights):\n    net = nn.Sequential()\n    net.add_module('conv1_1', Conv2d(in_channels=3, out_channels=32, kernel_size=3, bias=False, \n                                     padding=0, weight_init=weights['conv1_1_w']))\n    net.add_module('bn1_1', BatchNorm2d(num_features=32, weight_init=weights['bn1_1_w'], \n                                        bias_init=weights['bn1_1_b'], mean_init=weights['bn1_1_mean'],\n                                        var_init=weights['bn1_1_var']))\n    net.add_module('relu1_1', nn.ReLU(inplace=True))\n    net.add_module('conv1_2', Conv2d(in_channels=32, out_channels=32, kernel_size=3, bias=False, padding=0, \n                                     weight_init=weights['conv1_2_w']))\n    net.add_module('bn1_2', BatchNorm2d(num_features=32, weight_init=weights['bn1_2_w'], \n                                        bias_init=weights['bn1_2_b'], mean_init=weights['bn1_2_mean'],\n                                        var_init=weights['bn1_2_var']))\n    net.add_module('relu1_2', nn.ReLU(inplace=True))\n    net.add_module('mp1', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n\n    net.add_module('conv2_1', Conv2d(in_channels=32, out_channels=64, kernel_size=3, \n                                     dilation=1, bias=False, padding=0, weight_init=weights['conv2_1_w']))\n    net.add_module('bn2_1', BatchNorm2d(num_features=64, weight_init=weights['bn2_1_w'], \n                                        bias_init=weights['bn2_1_b'], mean_init=weights['bn2_1_mean'],\n                                        var_init=weights['bn2_1_var']))\n    net.add_module('relu2_1', nn.ReLU(inplace=True))\n    net.add_module('conv2_2', Conv2d(in_channels=64, out_channels=64, kernel_size=3, \n                                     dilation=1, bias=False, padding=0, weight_init=weights['conv2_2_w']))\n    net.add_module('bn2_2', BatchNorm2d(num_features=64, weight_init=weights['bn2_2_w'], \n                                        bias_init=weights['bn2_2_b'], mean_init=weights['bn2_2_mean'],\n                                        var_init=weights['bn2_2_var']))\n    net.add_module('relu2_2', nn.ReLU(inplace=True))\n    net.add_module('mp2', nn.MaxPool2d(kernel_size=3, stride=2, dilation=1, padding=0))\n\n    net.add_module('conv3_1', Conv2d(in_channels=64, out_channels=128, kernel_size=3, \n                                        dilation=1, bias=False, padding=0, weight_init=weights['conv3_1_w']))\n    net.add_module('bn3_1', BatchNorm2d(num_features=128, weight_init=weights['bn3_1_w'], \n                                        bias_init=weights['bn3_1_b'], mean_init=weights['bn3_1_mean'],\n                                        var_init=weights['bn3_1_var']))\n    net.add_module('relu3_1', nn.ReLU(inplace=True))\n    net.add_module('conv3_2', Conv2d(in_channels=128, out_channels=128, kernel_size=3, \n                                        dilation=1, bias=False, padding=0, weight_init=weights['conv3_2_w']))\n    net.add_module('bn3_2', BatchNorm2d(num_features=128, weight_init=weights['bn3_2_w'], \n                                           bias_init=weights['bn3_2_b'], mean_init=weights['bn3_2_mean'],\n                                        var_init=weights['bn3_2_var']))\n    net.add_module('relu3_2', nn.ReLU(inplace=True))\n    net.add_module('mp3', nn.MaxPool2d(kernel_size=3, stride=2, dilation=1, padding=0))\n    \n    # 'mp3' output has shape [batch_size,128, 9, 9]\n    net.add_module('flatten', Flatten())\n    net.add_module('fc1', Linear(in_features=128*9*9, out_features=512, bias=False, weight_init=weights['fc1_w']))\n    net.add_module('fc1_bn', BatchNorm1d(num_features=512, weight_init=weights['fc1_bn_w'], \n                                         bias_init=weights['fc1_bn_b'], mean_init=weights['fc1_bn_mean'],\n                                        var_init=weights['fc1_bn_var']))\n    net.add_module('fc1_relu', nn.ReLU(inplace=True))\n    net.add_module('fc2', Linear(in_features=512, out_features=1, bias=True, \n                                 weight_init=weights['fc2_w'], bias_init=weights['fc2_b']))\n    net.add_module('probs', nn.Sigmoid())\n    \n    net = net.train(False)\n    return net\n\ndef preproces(patches):\n    patches = patches.astype(np.float32)\n    patches = patches / 255 - 0.5\n    patches = patches.transpose(0,3,1,2)\n    return patches\n\ndef apply_net(input_data, net):\n    input_data = Variable(torch.FloatTensor(input_data))\n    output = net(input_data).data.numpy()\n    return output","metadata":{"cellId":"2wxm4b36s1jjt10tppfn9c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"net = create_network(weights)","metadata":{"cellId":"5plulfz2pvey0d17qdedzi"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = apply_net(preproces(patches), net)\npredictions = (predictions > 0.5).ravel().astype(np.int32)","metadata":{"cellId":"0tim37kffxyhpuk4e0gwy5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"predictions: {}\".format(predictions))\nprint(\"Accuracy: {}\".format((predictions == labels).mean()))\nprint(\"Road class accuracy: {}; Non-road class accuracy: {}\".format(np.mean(predictions[:32] == 1), \n                                                          np.mean(predictions[32:] == 0)))","metadata":{"cellId":"r8wa7goxv5z2kl8yi288d"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Your turn!\n\nYour task is to modify the network above to make it able to take image of arbitrary size as input and produce output of the same shape.\n\nMain changes:\n* Convert Linear layer to Conv2d (Don't forget to reshape weights from [n_out_features, n_in_features] to [n_out_filters, n_in_filters, kern_size, kern_sizse])\n* Replace BatchNorm1d with BatchNorm2d\n* Remove Flatten module\n* Remove strides from layers, add dilation in MaxPool2d and Conv2d (where it is needed)\n\nKnown troubles:\n* MaxPool2d wants padding value to be less then kernel_size/2. If you need bigger value (and you will!), add nn.ConstantPad2d(padding_size, 0) before MaxPool2d (and don't forget to set padding=0 in MaxPool2d)","metadata":{"cellId":"7ddgcadp0nbo8hfgvvfdic"}},{"cell_type":"code","source":"from layer_wrappers import *\n\ndef create_fully_conv_network(weights):\n    net = nn.Sequential()\n    # TODO\n    # it's better to start with copy-paste of 'create_network' function\n    net.add_module('probs', nn.Sigmoid())\n    net = net.train(False)\n    return net\n\ndef preproces(patches):\n    patches = patches.astype(np.float32)\n    patches = patches / 255 - 0.5\n    patches = patches.transpose(0,3,1,2)\n    return patches\n\ndef apply_net(input_data, net):\n    input_data = Variable(torch.FloatTensor(input_data))\n    output = net(input_data).data.numpy()\n    return output","metadata":{"cellId":"npbkgn7eqyr6c2xs766yf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"net2 = create_fully_conv_network(weights)","metadata":{"cellId":"wz1kgfboq6jiyk7kuzu9p"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = apply_net(preproces(patches[:5]), net2)\nassert predictions.shape[-2:] == patches.shape[1:3], \"{}, {}\".format(predictions.shape, patches.shape)","metadata":{"cellId":"z4qy2k7kibc2o1wh9d9wxt"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's visualize what we finally have got","metadata":{"cellId":"d1rjcg4z5wo45t4m3djq9"}},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\npatch_index = 1\nplt.subplot(1,4,1)\nplt.title('img')\nplt.imshow(patches[patch_index])\nplt.xticks([]); plt.yticks([])\nplt.subplot(1,4,2)\nplt.title('pred')\nplt.imshow(predictions[patch_index,0])\nplt.xticks([]); plt.yticks([])\nplt.subplot(1,4,3)\nplt.title('pred labels')\nplt.imshow(predictions[patch_index,0] > 0.5, 'gray')\nplt.xticks([]); plt.yticks([])\nplt.subplot(1,4,4)\nplt.title('blending')\nplt.imshow(make_blending(patches[patch_index], predictions[patch_index,0] > 0.5))\nplt.xticks([]); plt.yticks([])","metadata":{"cellId":"84l2kmvrn274ajee2xxvjc"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If everything is fine, you should be able to predict output for input image of any shape. Try crop 256x256 (or smaller if it doesn't fit in memory)","metadata":{"cellId":"gcozk12aiyry5u3tuq58y"}},{"cell_type":"code","source":"plt.imshow(im)\nplt.xticks([]); plt.yticks([])\nprint(im.shape)","metadata":{"cellId":"4pti55lmm9nyuwd3xwmoej"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"patch = im[:, 200:200+256]\npredictions = apply_net(preproces(patch[np.newaxis]), net2)\npredictions.shape, patch.shape","metadata":{"cellId":"9w9oqb23y8t8l8bhyvaprr"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.subplot(1,4,1)\nplt.title('img')\nplt.imshow(patch)\nplt.xticks([]); plt.yticks([])\nplt.subplot(1,4,2)\nplt.title('pred')\nplt.imshow(predictions[0,0])\nplt.xticks([]); plt.yticks([])\nplt.subplot(1,4,3)\nplt.title('pred labels')\nplt.imshow(predictions[0,0] > 0.5, 'gray')\nplt.xticks([]); plt.yticks([])\nplt.subplot(1,4,4)\nplt.title('blending')\nplt.imshow(make_blending(patch, predictions[0,0] > 0.5))\nplt.xticks([]); plt.yticks([])","metadata":{"cellId":"5a5wm4hrm7rgqv02xykw6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_and_apply(net, image, patch_w=150, overlap=80):\n    n_patches = image.shape[1] // patch_w\n    labels = np.zeros(image.shape[:2], np.uint8)\n    for i in range(n_patches):\n        print(i,n_patches)\n        patch = image[:, max(0, i*patch_w-overlap): min((i+1)*patch_w+overlap, image.shape[1])]\n        extra_left = i*patch_w - max(0, i*patch_w-overlap)\n        extra_right = min(image.shape[1], (i+1)*patch_w+overlap) - (i+1)*patch_w\n        out = (apply_net(preproces(patch[np.newaxis]), net)[0,0] > 0.5).astype(np.uint8)\n        labels[:, i*patch_w: (i+1)*patch_w] = out[:,extra_left:-extra_right]\n    if n_patches*patch_w < image.shape[1]:\n        last_patch_size = image.shape[1] - n_patches*patch_w\n        patch = image[:,-patch_w:]\n        labels[:,-last_patch_size:] = (apply_net(preproces(patch[np.newaxis]), net)[0,0] > 0.5).astype(np.uint8)[:,-last_patch_size:]\n    return labels","metadata":{"cellId":"kx2gwacyubxdr6ldb5df"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!M\nlabels = split_and_apply(net2, im)","metadata":{"cellId":"xdwf9ggub59tyjl18bp8e"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(make_blending(im, labels))\nplt.xticks([]); plt.yticks([])","metadata":{"cellId":"c1smg0bkvd50o9syp5wbhff"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"cellId":"y376brtwdwnsieudgzj7y"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### TL;DR DeepLab v2 architecture:\n![alt text](https://miro.medium.com/max/1283/1*8Lg66z7e7ijuLmSkOzhYvA.png \"Somthing wrong with image\")","metadata":{"cellId":"27tgjcdaitw7em8dg13jic"}},{"cell_type":"code","source":"","metadata":{"cellId":"h3750jt5foe0v684hs3rcda"},"outputs":[],"execution_count":null}]}