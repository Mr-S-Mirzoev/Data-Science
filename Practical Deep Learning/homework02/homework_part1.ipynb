{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "h4jkubmprqph396kkzha9e"
   },
   "source": [
    "# Homework part I\n",
    "\n",
    "The first problem set contains basic tasks in pytorch.\n",
    "\n",
    "__Note:__ Instead of doing this part of homework, you can prove your skills otherwise:\n",
    "* A commit to pytorch or pytorch-based repos will do;\n",
    "* Fully implemented seminar assignment in tensorflow or theano will do;\n",
    "* Your own project in pytorch that is developed to a state in which a normal human can understand and appreciate what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "hlx142siqy73qnmurugg6p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "zb3mxd9m35dbut30ow3omf"
   },
   "source": [
    "### Task I - tensormancy\n",
    "\n",
    "![img](https://media.giphy.com/media/3o751UMCYtSrRAFRFC/giphy.gif)\n",
    "\n",
    "When dealing with more complex stuff like neural network, it's best if you use tensors the way samurai uses his sword. \n",
    "\n",
    "\n",
    "__1.1 the cannabola__ \n",
    "[_disclaimer_](https://gist.githubusercontent.com/justheuristic/e2c1fa28ca02670cabc42cacf3902796/raw/fd3d935cef63a01b85ed2790b5c11c370245cbd7/stddisclaimer.h)\n",
    "\n",
    "Let's write another function, this time in polar coordinates:\n",
    "$$\\rho(\\theta) = (1 + 0.9 \\cdot cos (8 \\cdot \\theta) ) \\cdot (1 + 0.1 \\cdot cos(24 \\cdot \\theta)) \\cdot (0.9 + 0.05 \\cdot cos(200 \\cdot \\theta)) \\cdot (1 + sin(\\theta))$$\n",
    "\n",
    "\n",
    "Then convert it into cartesian coordinates ([howto](http://www.mathsisfun.com/polar-cartesian-coordinates.html)) and plot the results.\n",
    "\n",
    "Use torch tensors only: no lists, loops, numpy arrays, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "63tmtp95mfr8zny5t1m7d7"
   },
   "outputs": [],
   "source": [
    "theta = torch.linspace(- np.pi, np.pi, steps=1000)\n",
    "\n",
    "# compute rho(theta) as per formula above\n",
    "rho = ### YOUR CODE\n",
    "\n",
    "# Now convert polar (rho, theta) pairs into cartesian (x,y) to plot them.\n",
    "x = ### YOUR CODE\n",
    "y = ### YOUR CODE\n",
    "\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "plt.fill(x.numpy(), y.numpy(), color='green')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "m2zxz949f5qpsrww44wk"
   },
   "source": [
    "### Task II: the game of life\n",
    "\n",
    "Now it's time for you to make something more challenging. We'll implement Conway's [Game of Life](http://web.stanford.edu/~cdebs/GameOfLife/) in _pure pytorch_. \n",
    "\n",
    "While this is still a toy task, implementing game of life this way has one cool benefit: __you'll be able to run it on GPU! __ Indeed, what could be a better use of your gpu than simulating game of life on 1M/1M grids?\n",
    "\n",
    "![img](https://cdn.tutsplus.com/gamedev/authors/legacy/Stephane%20Beniak/2012/09/11/Preview_Image.png)\n",
    "If you've skipped the url above out of sloth, here's the game of life:\n",
    "* You have a 2D grid of cells, where each cell is \"alive\"(1) or \"dead\"(0)\n",
    "* Any living cell that has 2 or 3 neighbors survives, else it dies [0,1 or 4+ neighbors]\n",
    "* Any cell with exactly 3 neighbors becomes alive (if it was dead)\n",
    "\n",
    "For this task, you are given a reference numpy implementation that you must convert to pytorch.\n",
    "_[numpy code inspired by: https://github.com/rougier/numpy-100]_\n",
    "\n",
    "\n",
    "__Note:__ You can find convolution in `torch.nn.functional.conv2d(Z,filters)`. Note that it has a different input format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "c8zcotrb7cuxvp5365pk3k"
   },
   "outputs": [],
   "source": [
    "from scipy.signal import correlate2d as conv2d\n",
    "\n",
    "def np_update(Z):\n",
    "    # Count neighbours with convolution\n",
    "    filters = np.array([[1,1,1],\n",
    "                        [1,0,1],\n",
    "                        [1,1,1]])\n",
    "    \n",
    "    N = conv2d(Z,filters,mode='same')\n",
    "    \n",
    "    # Apply rules\n",
    "    birth = (N==3) & (Z==0)\n",
    "    survive = ((N==2) | (N==3)) & (Z==1)\n",
    "    \n",
    "    Z[:] = birth | survive\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "uotp1vmmzh6b13v16ghyd"
   },
   "outputs": [],
   "source": [
    "def torch_update(Z):\n",
    "    \"\"\"\n",
    "    Implement an update function that does to Z exactly the same as np_update.\n",
    "    :param Z: torch.FloatTensor of shape [height,width] containing 0s(dead) an 1s(alive)\n",
    "    :returns: torch.FloatTensor Z after updates.\n",
    "    \n",
    "    You can opt to create new tensor or change Z inplace.\n",
    "    \"\"\"\n",
    "    \n",
    "    #<Your code here!>\n",
    "    \n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "bdjcrg5r4klgc63y833iko"
   },
   "outputs": [],
   "source": [
    "#initial frame\n",
    "Z_numpy = np.random.choice([0,1],p=(0.5,0.5),size=(100,100))\n",
    "Z = torch.from_numpy(Z_numpy).type(torch.FloatTensor)\n",
    "\n",
    "#your debug polygon :)\n",
    "Z_new = torch_update(Z.clone())\n",
    "\n",
    "#tests\n",
    "Z_reference = np_update(Z_numpy.copy())\n",
    "assert np.all(Z_new.numpy() == Z_reference), \"your pytorch implementation doesn't match np_update. Look into Z and np_update(ZZ) to investigate.\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "r0o9rikxj53d5v8h3mz4r"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "\n",
    "#initialize game field\n",
    "Z = np.random.choice([0,1],size=(100,100))\n",
    "Z = torch.from_numpy(Z).type(torch.FloatTensor)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fig.show()\n",
    "\n",
    "for _ in range(100):\n",
    "    \n",
    "    #update\n",
    "    Z = torch_update(Z)\n",
    "    \n",
    "    #re-draw image\n",
    "    ax.clear()\n",
    "    ax.imshow(Z.numpy(),cmap='gray')\n",
    "    fig.canvas.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "mwj9qklh3vbma1e2u1irl"
   },
   "outputs": [],
   "source": [
    "#Some fun setups for your amusement\n",
    "\n",
    "#parallel stripes\n",
    "Z = np.arange(100)%2 + np.zeros([100,100])\n",
    "#with a small imperfection\n",
    "Z[48:52,50]=1\n",
    "\n",
    "Z = torch.from_numpy(Z).type(torch.FloatTensor)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fig.show()\n",
    "\n",
    "for _ in range(100):\n",
    "    Z = torch_update(Z)\n",
    "    ax.clear()\n",
    "    ax.imshow(Z.numpy(),cmap='gray')\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "bz8hofx4ah6yly383d75ih"
   },
   "source": [
    "More fun with Game of Life: [video](https://www.youtube.com/watch?v=C2vgICfQawE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "9fftxzcnnzlqhtitzypkdp"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Task III: Going deeper\n",
    "<img src=\"http://download.gamezone.com/uploads/image/data/1190338/article_post_width_a88.jpg\" width=360>\n",
    "Your third trial is to build your first neural network [almost] from scratch and pure torch.\n",
    "\n",
    "This time you will solve yet another digit recognition problem, but at a greater scale\n",
    "* 10 different letters\n",
    "* 20k samples\n",
    "\n",
    "We want you to build a network that reaches at least 80% accuracy and has at least 2 linear layers in it. Naturally, it should be nonlinear to beat logistic regression. You can implement it with either \n",
    "\n",
    "\n",
    "With 10 classes you will need to use __Softmax__ at the top instead of sigmoid and train for __categorical crossentropy__  (see [here](https://www.kaggle.com/wiki/LogLoss)).  Write your own loss or use `torch.nn.functional.nll_loss`. Just make sure you understand what it accepts as an input.\n",
    "\n",
    "Note that you are not required to build 152-layer monsters here. A 2-layer (one hidden, one output) neural network should already give you an edge over logistic regression.\n",
    "\n",
    "\n",
    "__[bonus kudos]__\n",
    "If you've already beaten logistic regression with a two-layer net, but enthusiasm still ain't gone, you can try improving the test accuracy even further! It should be possible to reach 90% without convnets.\n",
    "\n",
    "__SPOILERS!__\n",
    "At the end of the notebook you will find a few tips and frequent errors. \n",
    "If you feel confident enogh, just start coding right away and get there ~~if~~ once you need to untangle yourself. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "400hh7pcv48libce8uhc6b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from notmnist import load_notmnist\n",
    "X_train, y_train, X_test, y_test = load_notmnist(letters='ABCDEFGHIJ')\n",
    "X_train, X_test = X_train.reshape([-1, 784]), X_test.reshape([-1, 784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "eti17w8s17aigr8s7jkl0s"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=[12,4])\n",
    "for i in range(20):\n",
    "    plt.subplot(2,10,i+1)\n",
    "    plt.imshow(X_train[i].reshape([28,28]))\n",
    "    plt.title(str(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "plf5xtr48ek7kfg9y3tf"
   },
   "outputs": [],
   "source": [
    "#< a whole lot of your code > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "acg3axxegkqvq6i6sy7jw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "nzlf55u49j822e35jmrm4"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# SPOILERS!\n",
    "\n",
    "Recommended pipeline\n",
    "\n",
    "* Adapt logistic regression from week2 seminar assignment to classify one letter against others (e.g. A vs the rest)\n",
    "* Generalize it to multiclass logistic regression.\n",
    "  - Either try to remember lecture 0 or google it.\n",
    "  - Instead of weight vector you'll have to use matrix (feature_id x class_id)\n",
    "  - softmax (exp over sum of exps) can implemented manually or as nn.Softmax (layer) F.softmax (function)\n",
    "  - probably better to use STOCHASTIC gradient descent (minibatch) for greater speed\n",
    "    - you can also try momentum/rmsprop/adawhatever\n",
    "    - in which case sample should probably be shuffled (or use random subsamples on each iteration)\n",
    "* Add a hidden layer. Now your logistic regression uses hidden neurons instead of inputs.\n",
    "  - Hidden layer uses the same math as output layer (ex-logistic regression), but uses some nonlinearity (e.g. sigmoid) instead of softmax\n",
    "  - You need to train both layers, not just output layer :)\n",
    "  - __Do not initialize weights with zeros__ (due to symmetry effects). A gaussian noize with small variance will do.\n",
    "  - 50 hidden neurons and a sigmoid nonlinearity will do for a start. Many ways to improve. \n",
    "  - In ideal casae this totals to 2 .dot's, 1 softmax and 1 sigmoid\n",
    "  - __make sure this neural network works better than logistic regression__\n",
    "  \n",
    "* Now's the time to try improving the network. Consider layers (size, neuron count),  nonlinearities, optimization methods, initialization - whatever you want, but please avoid convolutions for now.\n",
    "  \n",
    "* If anything seems wrong, try going through one step of training and printing everything you compute.\n",
    "* If you see NaNs midway through optimization, you can estimate log P(y|x) as via F.log_softmax(layer_before_softmax)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "notebookId": "e5288bf7-d6a2-4273-865c-cc30559f7815"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
