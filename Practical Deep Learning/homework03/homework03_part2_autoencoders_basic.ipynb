{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ivhn3p0b82j8baf12mqui4",
    "colab_type": "text",
    "id": "VBNHzOrf_eWj"
   },
   "source": [
    "### Denoising Autoencoders And Where To Find Them\n",
    "\n",
    "Today we're going to train deep autoencoders and deploy them to faces and search for similar images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "alix1dvo0igqgv1krfg0cn"
   },
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "6kwdnhvfhqjo34nr3ntghh"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "from gfile import download_list\n",
    "\n",
    "download_list(\n",
    "    url='https://drive.google.com/file/d/1F96x4LDbsTZGMMq81fZr7aduJCe8N95O',\n",
    "    filename='celeba.zip',\n",
    "    target_dir='.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "c6wab9m94ah0f1aioyiu5"
   },
   "outputs": [],
   "source": [
    "#!L:bash\n",
    "unzip celeba.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "fe186e8ync31si0hdxa2c",
    "colab": {},
    "colab_type": "code",
    "id": "o7jFIMZd_eWp"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "LATENT_DIMENSION = 4\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "eolyp3wge2suqrm4g6rwad"
   },
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "otrfn0l8zsnhlqnz3jn5e",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "class CropCelebA64:\n",
    "    \n",
    "    def __call__(self, pic):\n",
    "        new_pic = pic.crop((15, 40, 178 - 15, 218 - 30))\n",
    "        return new_pic\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "easim8dmyaptymmmevurmi"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "train_dataset = torchvision.datasets.CelebA(\n",
    "    root='celeba',\n",
    "    split='train',\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        CropCelebA64(),\n",
    "        torchvision.transforms.Resize(64),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        \n",
    "    ]),\n",
    ")\n",
    "\n",
    "validation_dataset = torchvision.datasets.CelebA(\n",
    "    root='celeba',\n",
    "    split='valid',\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        CropCelebA64(),\n",
    "        torchvision.transforms.Resize(64),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        \n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3co9x9e888ev0vweoilwr"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "sp7wai809npy0ecwxbrz0m",
    "colab": {},
    "colab_type": "code",
    "id": "3fAdhPn2_eWy"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "samples = torch.stack([train_dataset[i][0] for i in range(32, 48)], dim=0)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(make_grid(samples, nrow=4).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "qmw2xsv31loqvtx3h38wy",
    "colab_type": "text",
    "id": "csBv6bf1_eW7"
   },
   "source": [
    "### Autoencoder architecture\n",
    "\n",
    "Let's design autoencoder as a single lasagne network, going from input image through bottleneck into the reconstructed image.\n",
    "\n",
    "<img src=\"http://nghiaho.com/wp-content/uploads/2012/12/autoencoder_network1.png\" width=640px>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vhjnb4qozdkoatdpb28j5",
    "colab_type": "text",
    "id": "O6fFezL-_eW8"
   },
   "source": [
    "## First step: PCA\n",
    "\n",
    "Principial Component Analysis is a popular dimensionality reduction method. \n",
    "\n",
    "Under the hood, PCA attempts to decompose object-feature matrix $X$ into two smaller matrices: $W$ and $\\hat W$ minimizing _mean squared error_:\n",
    "\n",
    "$$\\|(X W) \\hat{W} - X\\|^2_2 \\to_{W, \\hat{W}} \\min$$\n",
    "- $X \\in \\mathbb{R}^{n \\times m}$ - object matrix (**centered**);\n",
    "- $W \\in \\mathbb{R}^{m \\times d}$ - matrix of direct transformation;\n",
    "- $\\hat{W} \\in \\mathbb{R}^{d \\times m}$ - matrix of reverse transformation;\n",
    "- $n$ samples, $m$ original dimensions and $d$ target dimensions;\n",
    "\n",
    "In geometric terms, we want to find d axes along which most of variance occurs. The \"natural\" axes, if you wish.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/PCA_fish.png/256px-PCA_fish.png)\n",
    "\n",
    "\n",
    "PCA can also be seen as a special case of an autoencoder.\n",
    "\n",
    "* __Encoder__: X -> Dense(d units) -> code\n",
    "* __Decoder__: code -> Dense(m units) -> X\n",
    "\n",
    "Where Dense is a fully-connected layer with linear activaton:   $f(X) = W \\cdot X + \\vec b $\n",
    "\n",
    "\n",
    "Note: the bias term in those layers is responsible for \"centering\" the matrix i.e. substracting mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "rchezifextajq2chzq1cvq"
   },
   "source": [
    "**Hint**: you may need nn.Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "4byy90ev5q4gsqzyjzmgc",
    "colab": {},
    "colab_type": "code",
    "id": "gB5hwVLe_eW_"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "class PCAAutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Here we define a simple linear autoencoder as described above.\n",
    "    We also flatten and un-flatten data to be compatible with image shapes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, code_size=32):\n",
    "        super(PCAAutoEncoder, self).__init__()\n",
    "        \n",
    "        self.enc = # <Your code: define encoder layer>\n",
    "        self.dec = # <Your code: define decoder layer>\n",
    "    \n",
    "    def batch_loss(self, batch):\n",
    "        reconstruction = # <Your code: define reconstruction object>\n",
    "        return torch.mean((batch - reconstruction) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "fqescddeugpayszyqvlsz6",
    "colab_type": "text",
    "id": "69_Da_I7_eXB"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "As usual, iterate minibatches of data and call train_step, then evaluate loss on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "8tp67ngwi2us3s9p2rcdf",
    "colab": {},
    "colab_type": "code",
    "id": "7sbvg3Z__eXD"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def train(model, dataset, num_epoch=32):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adamax(model.parameters(), lr=0.002)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        losses = []\n",
    "        \n",
    "        for i, (batch, _) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.batch_loss(batch.to(device))\n",
    "            loss.backward()\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"#{epoch + 1}, Train loss: {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "te4e49bt2fatl6rkhkvpxe",
    "colab": {},
    "colab_type": "code",
    "id": "wj7YPamq_eXF"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def visualize(img, model):\n",
    "    \"\"\"Draws original, encoded and decoded images\"\"\"\n",
    "    code = model.enc(img[None].to(device)\n",
    "    reco = model.dec(code)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Original\")\n",
    "    plt.imshow(img.cpu().numpy().transpose([1, 2, 0]).clip(0, 1))\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Code\")\n",
    "    plt.imshow(code.cpu().detach().numpy().reshape([code.shape[-1] // 2, -1]))\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Reconstructed\")\n",
    "    plt.imshow(reco[0].cpu().detach().numpy().transpose([1, 2, 0]).clip(0, 1))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "2i2m9y9qsq9p1p94qh388m",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "4jkVqhLd_eXI",
    "outputId": "8dc6d9eb-3586-4c29-c0cb-7bee25e55d13"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "aenc = PCAAutoEncoder()\n",
    "train(aenc, train_dataset, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "2kfx9ik4in8q57i0eb7ebd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RqTUvLp8_eXM",
    "outputId": "1ab49844-7289-4d63-d6cc-78dbe1590985"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "dataloader_test = DataLoader(validation_dataset, batch_size=BATCH_SIZE)\n",
    "scores = []\n",
    "\n",
    "for i, (batch, _) in enumerate(dataloader_test):\n",
    "    scores.append(aenc.batch_loss(batch.to(device)).data.cpu().numpy())\n",
    "\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "navjf4d0szjyetjlnbmtzr",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1332
    },
    "colab_type": "code",
    "id": "cvljk13x_eXP",
    "outputId": "6567bcc1-4089-4d01-c05d-8fad2e885a35"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "for i in range(5):\n",
    "    img = validation_dataset[i][0]\n",
    "    visualize(img, aenc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "msghod3ggmkac5pl7fb4",
    "colab_type": "text",
    "id": "NIurZSFN_eXV"
   },
   "source": [
    "### Going deeper\n",
    "\n",
    "PCA is neat but surely we can do better. This time we want you to build a deep autoencoder by... stacking more layers.\n",
    "\n",
    "In particular, your encoder and decoder should be at least 3 layers deep each. You can use any nonlinearity you want and any number of hidden units in non-bottleneck layers provided you can actually afford training it.\n",
    "\n",
    "![layers](https://pbs.twimg.com/media/CYggEo-VAAACg_n.png:small)\n",
    "\n",
    "A few sanity checks:\n",
    "* There shouldn't be any hidden layer smaller than bottleneck (encoder output).\n",
    "* Don't forget to insert nonlinearities between intermediate dense layers.\n",
    "* Convolutional layers are good idea. To undo convolution use nn.Upsample + nn.Conv2d\n",
    "* Adding activation after bottleneck is allowed, but not strictly necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "uc3uy1ezj0aabpo2bbl4vc",
    "colab": {},
    "colab_type": "code",
    "id": "IGP0HAr3_eXW"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "class DeepPCAAutoEncoder(nn.Module):\n",
    "    def __init__(self, code_size=32):\n",
    "        super(DeepPCAAutoEncoder, self).__init__()\n",
    "        \n",
    "        self.enc = #<Your code: define encoder as per instructions above>\n",
    "        self.dec = #<Your code: define decoder as per instructions above>\n",
    "    \n",
    "    def batch_loss(self, batch):\n",
    "        reconstruction = #<Your code: define reconstruction object>\n",
    "        return torch.mean((batch - reconstruction)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ib0qxjwcxe7mmam7h33mb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "vtSWYDsB_eXc",
    "outputId": "55caa8ab-7c68-46b8-9743-0f28b3311469"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "aenc_deep = DeepPCAAutoEncoder()\n",
    "train(aenc_deep, train_dataset, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "flbi9n8fjcnojqdqx3jpr",
    "colab_type": "text",
    "id": "tLkLF1hC_eXh"
   },
   "source": [
    "Training may take long, it's okay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "6u2t3ak11e7l5nljm580zn",
    "colab_type": "text",
    "id": "twGO5gAa4n7Y"
   },
   "source": [
    "**Check autoencoder shapes along different code_sizes. Check architecture of you encoder-decoder network is correct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ks1tos96qtrqcigokty5h",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "HmR7ot5__eXi",
    "outputId": "46eaef3b-ab3d-4d55-8d51-0c2d35c90533"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def get_dim(layer): return np.prod(layer.output_shape[1:])\n",
    "\n",
    "\n",
    "for code_size in [1, 8, 32, 128, 512, 1024]:\n",
    "    help_tensor = next(iter(DataLoader(train_dataset, batch_size=BATCH_SIZE)))\n",
    "    model = DeepPCAAutoEncoder(code_size).to(device)\n",
    "    encoder_out = model.enc(help_tensor.to(device))\n",
    "    decoder_out = model.dec(encoder_out)\n",
    "\n",
    "    print(\"Testing code size %i\" % code_size)\n",
    "\n",
    "    assert encoder_out.shape[1:] == torch.Size(\n",
    "        [code_size]), \"encoder must output a code of required size\"\n",
    "    assert decoder_out.shape[1:] == img_shape, \"decoder must output an image of valid shape\"\n",
    "    assert len(list(model.dec.children())) >= 6,  \"decoder must contain at least 3 dense layers\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ogx198u7yontgjjkk92w9d",
    "colab_type": "text",
    "id": "GiNYsxJQ_eXk"
   },
   "source": [
    "__Hint:__ if you're getting \"Encoder layer is smaller than bottleneck\" error, use code_size when defining intermediate layers. \n",
    "\n",
    "For example, such layer may have code_size*2 units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "1zb4itw157kh5z91oxyt74",
    "colab_type": "text",
    "id": "tkJCskEvyixo"
   },
   "source": [
    "** Lets check you model's score. You should beat value of 0.005 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "xlylcsj1ac83shb7kno6w6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1349
    },
    "colab_type": "code",
    "id": "khcs90Yi_eXl",
    "outputId": "ec79444d-7be6-4a7d-9a87-3f7d60b8aa10"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "dataloader_test = DataLoader(validation_dataset, batch_size=BATCH_SIZE)\n",
    "scores = []\n",
    "for i, (batch) in enumerate(dataloader_test):\n",
    "    scores.append(aenc_deep.batch_loss(batch.to(device)).data.cpu().numpy())\n",
    "    encoder_out = aenc_deep.enc(batch.to(device))\n",
    "\n",
    "reconstruction_mse = np.mean(scores)\n",
    "\n",
    "assert reconstruction_mse <= 0.005, \"Compression is too lossy. See tips below.\"\n",
    "assert len(encoder_out.shape) == 2 and encoder_out.shape[1] == 32, \\\n",
    "    \"Make sure encoder has code_size units\"\n",
    "\n",
    "print(\"Final MSE:\", reconstruction_mse)\n",
    "for i in range(5):\n",
    "    img = validation_dataset[i][0]\n",
    "    visualize(img, aenc_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "9sklavp61yafqxdj4spgur",
    "colab_type": "text",
    "id": "KNlhwrtc_eXo"
   },
   "source": [
    "__Tips:__ If you keep getting \"Compression to lossy\" error, there's a few things you might try:\n",
    "\n",
    "* Make sure it converged. Some architectures need way more than 32 epochs to converge. They may fluctuate a lot, but eventually they're going to get good enough to pass. You may train your network for as long as you want.\n",
    "\n",
    "* Complexity. If you already have, like, 152 layers and still not passing threshold, you may wish to start from something simpler instead and go in small incremental steps.\n",
    "\n",
    "* Architecture. You can use any combination of layers (including convolutions, normalization, etc) as long as __encoder output only stores 32 numbers per training object__. \n",
    "\n",
    "A cunning learner can circumvent this last limitation by using some manual encoding strategy, but he is strongly recommended to avoid that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "h2b1hrhuleu11zpfxsct0y",
    "colab_type": "text",
    "id": "JQkFuzTz_eXp"
   },
   "source": [
    "## Denoising AutoEncoder\n",
    "\n",
    "Let's now make our model into a denoising autoencoder.\n",
    "\n",
    "We'll keep your model architecture, but change the way it trains. In particular, we'll corrupt it's input data randomly before each epoch.\n",
    "\n",
    "There are many strategies to apply noise. We'll implement two popular one: adding gaussian noise and using dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "t6jrcszmse60x3gcvnoox",
    "colab": {},
    "colab_type": "code",
    "id": "sQUS359N_eXq"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def apply_gaussian_noise(X,sigma=0.1):\n",
    "    \"\"\"\n",
    "    adds noise from normal distribution with standard deviation sigma\n",
    "    :param X: image tensor of shape [batch,height,width,3]\n",
    "    \"\"\"\n",
    "        \n",
    "    #<Your code: define noise>\n",
    "        \n",
    "    return X + noise\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "26ghgtcit33g91v1y3syit",
    "colab_type": "text",
    "id": "xy71ZmuPz1il"
   },
   "source": [
    "**noise tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "4vgmr1v9eyikw73286fwh",
    "colab": {},
    "colab_type": "code",
    "id": "pslPEzXS_eXs"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "X = torch.stack([train_dataset[i][0] for i in range(100)], dim=0)\n",
    "theoretical_std = (X.std() ** 2 + 0.5 ** 2) ** .5\n",
    "our_std = apply_gaussian_noise(X, sigma=0.5).std()\n",
    "\n",
    "assert abs(theoretical_std - our_std) < 0.01, \\\n",
    "    \"Standard deviation does not match it's required value. Make sure you use sigma as std.\"\n",
    "assert abs(apply_gaussian_noise(X, sigma=0.5).mean() - X.mean()) < 0.01, \\\n",
    "    \"Mean has changed. Please add zero-mean noise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "cg613319c99ij12vn8x6h",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "unMfBi8q_eXu",
    "outputId": "0f0f446f-ae38-481d-d7dd-8e0b1290795e"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(X[0].permute([1, 2, 0]))\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(apply_gaussian_noise(X[:1], sigma=0.01)[0].permute([1, 2, 0]).clip(0, 1))\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(apply_gaussian_noise(X[:1], sigma=0.1)[0].permute([1, 2, 0]).clip(0, 1))\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(apply_gaussian_noise(X[:1], sigma=0.5)[0].permute([1, 2, 0]).clip(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "umbhajxl4udn5e5ldcqqk",
    "colab": {},
    "colab_type": "code",
    "id": "9hHGK_Wr_eXx"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def train_noise(model, dataset, num_epoch=50):\n",
    "    # <Your code: define train function for denoising autoencoder as train function above>\n",
    "    # <Think carefully, what should be ground-truth image for computing loss function>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "iv9naunj5yzswfsmpvew"
   },
   "source": [
    "__Note:__ You may change the way the training with noise is done, if you want. For example, you may change Dataloader or batch_loss function in model and leave train function unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "gda36e90l3c21ff7mc93g",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "fVD2-ujS_eX8",
    "outputId": "a7757a50-c871-449a-f664-4d719e44b4ae"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "aenc = PCAAutoEncoder()\n",
    "train_noise(aenc, train_dataset, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ccn0u1gpltbzzhh8uwsm2",
    "colab_type": "text",
    "id": "mlekH4ww_eX_"
   },
   "source": [
    "__Note:__ if it hasn't yet converged, increase the number of iterations.\n",
    "\n",
    "__Bonus:__ replace gaussian noise with masking random rectangles on image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "kdiemzqs75mttwzbilr7n9",
    "colab_type": "text",
    "id": "HVj0NPXV3liL"
   },
   "source": [
    "**Let's evaluate!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "2i52mluzjsd729qv9epkoq",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1349
    },
    "colab_type": "code",
    "id": "pr1Drxb1_eX_",
    "outputId": "daec01b0-18df-4547-c86e-a18ed4dcac60"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "dataloader_test = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "scores = []\n",
    "\n",
    "for i, (batch) in enumerate(dataloader_test):\n",
    "    scores.append(aenc.batch_loss(batch.to(device)).data.cpu().numpy())\n",
    "    encoder_out = aenc.enc(batch.to(device))\n",
    "\n",
    "reconstruction_mse = np.mean(scores)\n",
    "\n",
    "print(\"Final MSE:\", reconstruction_mse)\n",
    "for i in range(5):\n",
    "    img = validation_dataset[i][0]\n",
    "    visualize(img, aenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "9j6a3pjq3lqwmcop92zqp",
    "colab_type": "text",
    "id": "weU6quCI_eYE"
   },
   "source": [
    "### Image retrieval with autoencoders\n",
    "\n",
    "So we've just trained a network that converts image into itself imperfectly. This task is not that useful in and of itself, but it has a number of awesome side-effects. Let's see it in action.\n",
    "\n",
    "First thing we can do is image retrieval aka image search. We we give it an image and find similar images in latent space. \n",
    "\n",
    "To speed up retrieval process, we shall use Locality-Sensitive Hashing on top of encoded vectors. We'll use scikit-learn's implementation for simplicity. In practical scenario, you may want to use [specialized libraries](https://erikbern.com/2015/07/04/benchmark-of-approximate-nearest-neighbor-libraries.html) for better performance and customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "88ym78lanvfx7xfd08524",
    "colab": {},
    "colab_type": "code",
    "id": "afiR-pC3_eYG"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "# encodes batch of images into a codes\n",
    "\n",
    "codes =  # <Your code:encode all images in train_dataset>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "zme8citmss9pyvtptk26l",
    "colab": {},
    "colab_type": "code",
    "id": "nojmuKtb_eYI"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "assert codes.shape[0] == len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "2li0ls8w5sntibbiqbjn3p",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "GfGatyHi_eYK",
    "outputId": "155b0183-c7e4-4bd5-fb01-e0886a5eab52"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "from sklearn.neighbors import LSHForest\n",
    "lshf = LSHForest(n_estimators=50).fit(codes.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "p3n3pzodoy0h9826v4rfen",
    "colab": {},
    "colab_type": "code",
    "id": "shw1V6Zn_eYP"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def get_similar(image, n_neighbors=5):\n",
    "    assert len(image.shape) == 3, \"image must be [batch,height,width,3]\"\n",
    "\n",
    "    code =  # <Your code: encode image into latent code>\n",
    "\n",
    "    (distances,), (idx,) =  # <Your code: using lshf.kneighbors find nearest neighbors>\n",
    "\n",
    "    return distances, train_dataset[idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "zoo8awv0lxc7cny05oezt",
    "colab": {},
    "colab_type": "code",
    "id": "5JkabL1A_eYQ"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def show_similar(image):\n",
    "\n",
    "    distances, neighbors = get_similar(image, n_neighbors=11)\n",
    "\n",
    "    plt.figure(figsize=[8, 6])\n",
    "    plt.subplot(3, 4, 1)\n",
    "    plt.imshow(image.cpu().numpy().transpose([1, 2, 0]))\n",
    "    plt.title(\"Original image\")\n",
    "\n",
    "    for i in range(11):\n",
    "        plt.subplot(3, 4, i+2)\n",
    "        plt.imshow(neighbors[i].cpu().numpy().transpose([1, 2, 0]))\n",
    "        plt.title(\"Dist=%.3f\" % distances[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "9a0xfscrfvt06xojzxmoksb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "colab_type": "code",
    "id": "VG_0tXSq_eYT",
    "outputId": "b47d1a24-4e97-4752-c5b3-e7a5f14cd304"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "show_similar(validation_dataset[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "5y11qhs847wd7t3lfoigoj",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "colab_type": "code",
    "id": "4Z25ZSQO_eYV",
    "outputId": "34a91c49-6cd7-4929-abc8-7734cadb0f55"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "show_similar(validation_dataset[500][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ms239pdnyeic9ter07u05n",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "colab_type": "code",
    "id": "uPyK6-vk_eYf",
    "outputId": "b4b7c73d-aa52-477e-fd0b-d12944804a89"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "show_similar(validation_dataset[66][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "dtu8f86cfwioatph69ui7",
    "colab_type": "text",
    "id": "piVrNWXZ_eYn"
   },
   "source": [
    "## Cheap image morphing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "9wkhbatr0rcvsuk9jb6nqf",
    "colab_type": "text",
    "id": "DYqDtg6K2z5e"
   },
   "source": [
    "Here you should take two full-sized objects, code it and obtain intermediate object by decoding an intermixture code.\n",
    "\n",
    "$Code_{mixt} = a1\\cdot code1 + a2\\cdot code2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "shihpiwyw1ln6d54hqha4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "colab_type": "code",
    "id": "IFDk4E7N_eYr",
    "outputId": "f2213955-faa5-4e28-b851-b04e5286095d"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "for _ in range(5):\n",
    "    image1, image2 =  # <Your code:choose two image randomly>\n",
    "\n",
    "    code1, code2 =  # <Your code:decode it>\n",
    "\n",
    "    plt.figure(figsize=[10, 4])\n",
    "    for i, a in enumerate(np.linspace(0, 1, num=7)):\n",
    "\n",
    "        output_code =  # <Your code:define intermixture code>\n",
    "\n",
    "        output_image = aenc.dec(output_code[None])[0]\n",
    "        plt.subplot(1, 7, i+1)\n",
    "        plt.imshow(output_image.cpu().detach().numpy().transpose([1, 2, 0]))\n",
    "        plt.title(\"a=%.2f\" % a)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xfvn8w2p62cqi4tmuahq5",
    "colab_type": "text",
    "id": "lKZTo47L_eYu"
   },
   "source": [
    "Of course there's a lot more you can do with autoencoders.\n",
    "\n",
    "If you want to generate images from scratch, however, we recommend you our honor track seminar about generative adversarial networks."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "autoencoders_collab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "notebookId": "2d2fb8a1-aa60-453d-bff0-09985600e40c"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
